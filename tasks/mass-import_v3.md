# Mass import v3

## Goal

The goal is when ever I find a new source of artworks or artists, I can build a scraper to capture that data, then import it into this projects database.

There are three parts to this project

1. Scraper

A script that is designed to scrape a spcific website for artworks and artist details. There will be many of these scripts each designed for a specific website.

The artwork details are saved as .GEOJSON file with a series of properties.

The arist details are saved as a JSON with a name, Biography, and a series of properties.

2. Mass-Import endpoint

This is the endpoint on the server that receives **individual** artists or artworks. The endpoint accepts GeoJSON format for the data item and performs simple operations: validation, database insertion, and response. It processes **one item at a time** and returns success/failure status for that single item.

The endpoint is stateless - it doesn't track import sessions or maintain statistics.

3. Mass-Import runner

This local CLI script takes the output from the scrapers (artwork.geojson, artist.json) and sends items to the Mass-Import endpoint **one artist or artwork at a time**.

The runner is responsible for:

- Reading and parsing the input file (GeoJSON or JSON)
- Iterating through each item in the file
- Sending individual items to the API endpoint
- **Tracking session statistics** (total items, success count, failure count, timing)
- Generating comprehensive reports with per-item results
- Managing checkpoints for resumable imports
- Handling retries and error recovery

Most of the orchestration work is done here.

This is a CLI tool that runs with a configuration file.

## Files

These files are generated by the scrapers, and used by the Mass-Import runner

### Artwork GEO JSON file

The GEO JSON file has a few key elements.

- "id": This is the ID for the system, this is a unique identifier across all artworks in the systems. Its generated from other properties in the artwork.
- "type": Hard coded to "Feature"
- "geometry": This is of type "Point" and contains the coordinates of the artwork.
- "properties": This is a list of tags or properties of the artwork. There are some specific properties that should be part of all artworks. There will be properties that are free form that do not match the predefine set.
  - "source": This is where the artwork data was scraped from for this import. Its the root domain.
  - "source_url": This is the exact url that was scraped to get the data for this specific artwork.
  - "title": This is the name of the artwork. Some websites will use 'name', or 'title'
  - "artwork_type": This is the type of artwork this is, this comes from an enumerated list, the import should be able to add to that enumerated list. Example: "sculpture", "digital media", etc...
  - "description": This is a text field of the description of the artwork. This could be very long text. This is markdown.
  - "photos": An array of photo urls.
  - "artist": The name of the artist. Note, this will be looked up to see if we already have that artist in our collection, if we do then we will link this artwork with that artist.

Note: Other properties like "owner", "material", etc... are in the property list and will be sent to the mass-import endpoint.

```json
{
      "type": "Feature",
      "id": "node/publicart117",
      "geometry": {
        "type": "Point",
        "coordinates": [
          -123.003613,
          49.225237
        ]
      },
      "properties": {
        "source": "https://burnabyartgallery.ca",
        "source_url": "https://collections.burnabyartgallery.ca/link/publicart117",
        "title": "blacktail",
        "artwork_type": "sculpture",
        "location": "MetroPlace – 6461 Telford Ave., Burnaby",
        "start_date": "2015",
        "material": "aluminum",
        "technique": "cast aluminum",
        "keywords": "public art, sculpture, private development, nature, animal",
        "owner": "Intracorp",
        "category": "City of Burnaby Public Art Collection",
        "accession number": "PA.2015",
        "collection": "Public Art Registry",
        "description": "Elegant and elemental forms rest on nearby plazas, as if they were shed by a mythical deer. Each form creates a space. blacktail, lies upright like an open hand, providing a place to rest. Two blocks away, shed, an inversion of blacktail, invites you into its shelter, like a child playing in a fort in the forest. These sculptural forms imply an entire landscape, yet also convey an intimacy. Together they are a play of form and meaning; the forms identical yet seeming entirely different.”",
        "photos": [
          "https://collections.burnabyartgallery.ca/media/hpo/_Data/_Art_Gallery/_Unrestricted/2014/NA/NA_2014_PPP_blacktail_crop.jpg",
          "https://collections.burnabyartgallery.ca/media/hpo/_Data/_Art_Gallery/_Unrestricted/2014/NA/NA_2014_PPP_blacktail.jpg"
        ],
        "city": "burnaby",
        "country": "Canada",
        "province": "British Columbia",
        "artist": "Muse Atelier"
      }
    },
```

### Artist.json file

Fields

- "id": This is the ID for the system, this is a unique identifier across all artist in the systems. Its generated from other properties in the artist object.
- "artist": This is the name of the artist
- "properties": This is a list of tags or properties of the artist. There are some specific properties that should be part of all artist. There will be properties that are free form that do not match the predefine set.
  - "source": This is where the artist data was scraped from for this import. Its the root domain.
  - "source_url": This is the exact url that was scraped to get the data for this specific artist.
  - "description": This is a text field of the Biography of the artist. This could be very long text. This is markdown.

```json
{
  "artist": "Muse Atelier",
  "id": "node/publicart117",
  "type": "Artist",
  "properties": {
    "source": "https://burnabyartgallery.ca",
    "source_url": "https://collections.burnabyartgallery.ca/link/artists1307",
    "description": "Example Example Example Example Example Example Example Example Example ",
    "birth date": "1928",
    "death date": "2016",
    "website": "http://www.example.com"
  }
}
```

## Location

We need GPS infomration for all artwork but sometimes we only get an address. We need the ability to convert the address into a GPS location. For artworks where we only get the GPS location, we also need to get the location information.

There are two services here. FromGPSToLocation, FromLocationToGPS

Location information needs to be cached.

See src\lib\location\README.md for more information

## Mass-import runner

This is a CLI tool that takes a input file of a artwork.geojson or artist.json file.

```mass-import-runner --input <input file> --output <api|console> --config config.json --limit 10 --offset 0```

### Config

The mass importer runner needs a configuration file. In this configuration file it lists the http endpoint that will be used. There will be a seperate endpoint for the local system or the production system. Changing the path to this configuration file is how the user indicates where the import should be sent to.

### Output

The output option should allow the user to define if they want the runner to output to the API or to the console. The user might use the console for debugging purposes.

### Photos

The photos from the input file should be downloaded and caches on the local computer, the file name is replaced with the hash of the file. Then the file is sent to the server as part of the import.

The raw files should be stored in the r2 /originals/ folder, resized versions are stored in the /artwork/ folder

The artwork images cache should be warmed.

---

## Design Decisions (v3)

### Question 1: Primary Identifier for Artworks

**Answer: B** - The `id` field provided directly from the source data (e.g., `node/publicart117`).

### Question 2: Artist Management

**Answer:** There is an existing `artists` table in the database. When an artwork is imported with an artist's name:

1. Search the `artists` table for a matching artist by name
2. If the artist exists, link the artwork to that artist via the `artwork_artists` linking table
3. If the artist doesn't exist, create a new artist record, then link it

**Database Schema Reference:**

- `artists` table: Contains artist profiles with name, description (bio), aliases, tags
- `artwork_artists` table: Many-to-many linking table with `artwork_id`, `artist_id`, and `role`

### Question 3: CLI Structure

**Answer: B** - A single script that takes a configuration file defining the importer and its settings.

### Question 4: Photo Processing

**Answer: Updated** - The CLI runner downloads photos from source URLs, then submits them to the API as binary data (multipart/form-data or base64-encoded).

**Photo Processing Pipeline:**

**CLI Runner Responsibilities:**

- Download photos from source URLs (from scraper output)
- Validate file format (JPEG, PNG, WebP) and size (max 15MB)
- Cache downloaded photos locally (hash-based filenames)
- Include photo binary data in API request (multipart/form-data)
- Handle download failures and retry logic
- Report download errors in session statistics

**API Endpoint Responsibilities:**

- Receive photo binary data from CLI (not URLs)
- Validate uploaded photos (format, size, dimensions)
- Store originals in R2 bucket: `artworks/YYYY/MM/DD/timestamp-uuid.ext`
- Generate thumbnails asynchronously (400px, 1024px, 1200px variants)
- Return photo URLs pointing to Cloudflare R2 CDN
- No third-party URL downloading (only process submitted files)

**Benefits:**

- API doesn't need to access third-party sites (simplifies security/networking)
- CLI can retry failed downloads without involving API
- Local photo cache speeds up re-imports
- Better error handling and reporting

**Reference:** See `docs/photo-processing.md` for complete photo pipeline documentation

### Location Services

The system includes a comprehensive location caching system for converting coordinates to addresses:

**Location Cache System:**

- SQLite-based cache using `better-sqlite3`
- Reverse geocoding via Nominatim API (respects 1 req/sec rate limit)
- Cache-first strategy to minimize API calls
- Pre-warming capability for batch imports
- Stores structured address components: city, state/province, country, suburb

**Data Enhancement:**

- Coordinates are automatically enhanced with human-readable location names
- Location data stored in artwork `tags`: `location_display_name`, `location_country`, `location_state`, `location_city`, `location_suburb`

**Reference:** See `docs/location-cache-system.md` for complete location services documentation

---

## Clarifying Questions - Set 2

**Question 5: How should duplicate detection work in v3?**
(B) Simple GPS-only check: If coordinates are within 5m of existing artwork, mark as duplicate. Use the artwork `id` field - if it exists in the database, skip it.

**Question 6: How should the CLI runner handle errors during batch imports?**

(A) **Fault-tolerant individual processing** (recommended): Process each artwork/artist individually with its own transaction. Log errors for failed items but continue processing the rest. Generate a detailed error report at the end showing which items succeeded and which failed with specific error messages.

**Question 7: What should the configuration file format be?**

(A) **JSON format** with sections for API endpoint, authentication token, batch size, duplicate detection settings, and timeout values. Example:

```json
{
  "api": {
    "endpoint": "https://api.publicartregistry.com/api/mass-import/v3",
    "token": "admin-token-here"
  },
  "processing": {
    "timeout": 60000
  }
}
```

**Question 8: How should the system handle missing or optional data fields?**

(A) **Flexible validation with required minimums**: Require only essential fields (coordinates, title/name, source URL). All other fields (description, photos, tags) are optional. Store whatever data is provided and let the API handle validation of field formats.

**Question 9: Should the mass-import system create artworks directly or use the submissions system?**

(A) **Direct creation with auto-approval**: Create artwork records directly in `approved` status, bypassing the moderation queue. This is appropriate for trusted mass imports from verified sources. Include admin audit trail via system user token.

---

## Clarifying Questions - Set 3

**Question 10: How should the system handle artwork IDs from different sources?**

(A) **Store source ID in tags with namespace prefix**: Store the original source ID (e.g., `node/publicart117`) in the tags field as `source_id` or `original_id`. Generate our own UUID for the database `id` field. This allows tracking the original reference while maintaining our own unique identifiers.

**Question 11: What should happen when importing an artwork with multiple artists?**

Create multiple linked artists in the artwork_artist table.

**Question 12: How should the CLI runner track import progress and support resumption?**

(A) **Progress tracking with checkpoint file**: Create a JSON checkpoint file that records which items have been processed. If the import is interrupted, the runner can resume from the last checkpoint, skipping already-processed items. Include timestamps, status (success/failure), and error messages for each item.

**Question 13: What validation should happen on the artwork coordinates?**

(A) **Range validation with optional bounds checking**: Validate that lat is between -90 and 90, lon is between -180 and 180.

**Question 14: How should the system handle photo download failures?**

(C) Skip the entire artwork if any photo fails to download.

**Question 15: What should the import report include?**

(A) **Comprehensive JSON report**: Generate a timestamped JSON file with sections for: summary statistics (total, created, updated, skipped, failed), detailed item-by-item results (ID, title, status, errors), duplicate detection results (matched artwork IDs and similarity scores), processing metrics (duration, items/second), and configuration used. Also generate a human-readable text summary.

**Question 16: How should the system handle artwork tags/metadata?**

(A) **Preserve all source tags in JSON structure**: Store all properties from the source in the artwork `tags` field as a JSON object, including custom fields like `material`, `owner`, `technique`, etc. Maintain the original property names from the source. Reserve certain field names for system use (e.g., `location_*`, `source`, `source_url`).

**Question 17: Should the CLI runner support dry-run mode?**

(C) No, users can test against a development environment instead.

**Question 18: How should the system handle artist data that comes separately from artwork data?**

(A) **Support separate artist import mode**: The CLI should accept either `artwork.geojson` or `artist.json` files. When importing artists separately, create/update artist records in the database. When importing artworks, use artist names to link to existing artists. Support a `--type` flag or auto-detect based on file format.

**Question 19: What authentication method should the CLI use for the API?**

(A) **Bearer token in config file**: Include an admin authentication token in the configuration file. The CLI sends this token in the `Authorization: Bearer <token>` header with each API request. The API validates that the token has admin/mass-import permissions.

**Question 20: How should the system handle rate limiting from the API?**

(A) **Automatic retry with exponential backoff**: If the API returns a 429 (Too Many Requests) response, the CLI should automatically wait and retry. Use the `Retry-After` header if provided, otherwise use exponential backoff starting at 1 second. Include a configurable maximum retry count (default: 3) and timeout.

**Question 21: Should the mass-import endpoint be versioned (v3)?**

(A) **Yes, create new versioned endpoint**: Create `/api/mass-import/v3` as a separate endpoint from v1/v2. This allows for clean separation, easier testing, and the ability to deprecate old versions later. Include version in the API response for tracking.

**Question 22: How should the system handle artwork descriptions and artist biographies?**

(A) **Store as Markdown with sanitization**: Accept Markdown-formatted text for descriptions and bios. The Mass-import runner should sanitize the Markdown to prevent XSS attacks (remove script tags, dangerous HTML). Store the sanitized Markdown in the database. Frontend can render it safely using a Markdown library.

**Question 23: What should happen when importing artwork at coordinates that already have an artwork?**

(A) **Check duplicate threshold first**: Use the duplicate detection algorithm (GPS + title similarity + artist matching). If similarity score is above threshold, skip and report as duplicate. If below threshold, allow the import as these may be legitimately different artworks at the same location (e.g., multiple sculptures in a plaza).

**Question 24: How should the CLI handle large import files (10,000+ artworks)?**

(B) Load entire file into memory and process all at once.

**Question 25: Should the system support updating existing artworks during import?**

(D) Always fail on duplicates. Create log

**Question 26: How should the system validate and handle artwork types (sculpture, mural, etc.)?**

(A) **Flexible acceptance with normalization**: Accept any artwork type from the source data. The API can maintain a list of common types for normalization (e.g., "Sculpture" → "sculpture") but should also accept custom types. Store the type in the tags field. Frontend can filter and display based on common types while still showing unique types.

**Question 27: What logging level and detail should the CLI provide?**

(B) Fixed logging level with no configuration. Verbose

**Question 28: How should the system handle timezone information for dates?**

(B) Store dates exactly as provided without conversion.

**Question 29: Should the mass-import system support rollback functionality?**

(C) No rollback support; use database backups instead.

---

## Data Structure Specifications

### Mass Import V3 API Endpoint

**Endpoint:** `POST /api/mass-import/v3`

**Purpose:** Process a single artwork or artist item. The endpoint is stateless and handles one item per request.

### API Request Format (Single Artwork)

The endpoint accepts GeoJSON Feature format for artworks:

```json
{
  "type": "Feature",
  "id": "node/publicart117",
  "geometry": {
    "type": "Point",
    "coordinates": [-123.003613, 49.225237]
  },
  "properties": {
    "source": "https://burnabyartgallery.ca",
    "source_url": "https://collections.burnabyartgallery.ca/link/publicart117",
    "title": "blacktail",
    "description": "Markdown description here...",
    "artwork_type": "sculpture",
    "material": "aluminum",
    "technique": "cast aluminum",
    "start_date": "2015",
    "owner": "Intracorp",
    "location": "MetroPlace – 6461 Telford Ave., Burnaby",
    "city": "burnaby",
    "province": "British Columbia",
    "country": "Canada",
    "artist": "Muse Atelier",
    "photos": [
      "https://example.com/photo1.jpg",
      "https://example.com/photo2.jpg"
    ]
  }
}
```

**Note:** For artworks with multiple artists, use comma-separated string or JSON array:

```json
{
  "properties": {
    "artist": "Artist One, Artist Two, Artist Three"
  }
}
```

or

```json
{
  "properties": {
    "artists": ["Artist One", "Artist Two", "Artist Three"]
  }
}
```

### API Request Format (Single Artist)

For artists, use a simplified JSON structure:

```json
{
  "type": "Artist",
  "id": "artist-muse-atelier",
  "name": "Muse Atelier",
  "description": "Artist biography in Markdown...",
  "properties": {
    "source": "https://burnabyartgallery.ca",
    "source_url": "https://collections.burnabyartgallery.ca/link/artists1307",
    "birth_date": "1928",
    "death_date": "2016",
    "website": "http://www.example.com"
  }
}
```

### API Response Format (Single Item)

The endpoint returns a simple success/failure response for the single item:

```json
{
  "success": true,
  "data": {
    "id": "uuid-generated-by-system",
    "sourceId": "node/publicart117",
    "type": "artwork",
    "status": "created",
    "title": "blacktail",
    "duplicateCheck": {
      "isDuplicate": false,
      "similarityScore": 0.3,
      "matchedArtworkId": null
    },
    "artists": [
      {
        "id": "uuid-artist-id",
        "name": "Muse Atelier",
        "status": "linked"
      }
    ],
    "photos": [
      {
        "url": "https://photos.publicartregistry.com/artworks/2025/10/14/timestamp-uuid.jpg",
        "originalUrl": "https://example.com/photo1.jpg",
        "status": "uploaded"
      }
    ]
  }
}
```

**Error Response:**

```json
{
  "success": false,
  "error": {
    "code": "PHOTO_DOWNLOAD_FAILED",
    "message": "Failed to download photo from URL",
    "details": {
      "url": "https://example.com/missing.jpg",
      "httpStatus": 404
    }
  }
}
```

### CLI Runner - Session Statistics

The CLI runner maintains session-level statistics and generates comprehensive reports:

**Session Statistics (tracked by runner):**

```json
{
  "sessionId": "import-2025-10-14-120000",
  "startTime": "2025-10-14T12:00:00Z",
  "endTime": "2025-10-14T12:45:23Z",
  "duration": "45m 23s",
  "inputFile": "./data/burnaby-artworks.geojson",
  "configFile": "./config/production.json",
  "statistics": {
    "totalItems": 150,
    "processed": 150,
    "successful": 143,
    "failed": 7,
    "skipped": 0,
    "artworks": {
      "total": 120,
      "created": 115,
      "failed": 5
    },
    "artists": {
      "total": 30,
      "created": 12,
      "linked": 18
    },
    "photos": {
      "downloaded": 240,
      "failed": 8
    }
  },
  "performance": {
    "itemsPerSecond": 3.3,
    "averageResponseTime": 303,
    "apiCalls": 150,
    "retries": 12
  }
}
```

### CLI Configuration File Format

```json
{
  "api": {
    "endpoint": "https://api.publicartregistry.com/api/mass-import/v3",
    "token": "admin-token-here",
    "timeout": 60000
  },
  "processing": {
    "maxRetries": 3,
    "retryDelayMs": 1000,
    "delayBetweenItems": 100
  },
  "duplicateDetection": {
    "enabled": true,
    "gpsThresholdMeters": 5
  },
  "location": {
    "cacheEnabled": true,
    "cachePath": "./location-cache.db",
    "enhanceWithLocationData": true
  },
  "logging": {
    "level": "verbose",
    "outputFile": "./import-logs/{timestamp}.log",
    "format": "text"
  },
  "boundaries": {
    "enabled": false,
    "minLat": 49.0,
    "maxLat": 50.0,
    "minLon": -123.5,
    "maxLon": -122.5
  }
}
```

### CLI Checkpoint File Format

The runner creates a checkpoint file to enable resumable imports:

```json
{
  "sessionId": "import-2025-10-14-120000",
  "inputFile": "./data/burnaby-artworks.geojson",
  "configFile": "./config/production.json",
  "startedAt": "2025-10-14T12:00:00Z",
  "lastCheckpoint": "2025-10-14T12:05:23Z",
  "totalItems": 150,
  "processedItems": 75,
  "items": [
    {
      "index": 0,
      "sourceId": "node/publicart117",
      "type": "artwork",
      "status": "success",
      "databaseId": "uuid-here",
      "title": "blacktail",
      "responseTime": 305,
      "timestamp": "2025-10-14T12:00:15Z"
    },
    {
      "index": 1,
      "sourceId": "node/publicart118",
      "type": "artwork",
      "status": "failed",
      "error": {
        "code": "PHOTO_DOWNLOAD_FAILED",
        "message": "Failed to download photo from URL"
      },
      "timestamp": "2025-10-14T12:00:30Z"
    }
  ]
}
```

### CLI Final Report Format

At the end of the import session, the runner generates a comprehensive report:

```json
{
  "sessionId": "import-2025-10-14-120000",
  "summary": {
    "inputFile": "./data/burnaby-artworks.geojson",
    "totalItems": 150,
    "successful": 143,
    "failed": 7,
    "duration": "45m 23s",
    "completedAt": "2025-10-14T12:45:23Z"
  },
  "statistics": {
    "artworks": {
      "total": 120,
      "created": 115,
      "failed": 5
    },
    "artists": {
      "total": 30,
      "created": 12,
      "linked": 18
    }
  },
  "failed": [
    {
      "index": 1,
      "sourceId": "node/publicart118",
      "title": "Missing Photos",
      "error": "Photo download failed",
      "details": {
        "url": "https://example.com/missing.jpg",
        "httpStatus": 404
      }
    }
  ],
  "performance": {
    "itemsPerSecond": 3.3,
    "totalApiCalls": 150,
    "totalRetries": 12
  }
}
```

---

## Clarifying Questions - Set 4: Mass-Import Endpoint

**Question 30: How should the endpoint determine if the incoming data is an artwork or artist?**

(A) **Check the `type` field**: If `type === "Feature"` it's an artwork (GeoJSON), if `type === "Artist"` it's an artist. This provides explicit type declaration and follows GeoJSON standards.

**Question 31: Should the endpoint validate the GeoJSON structure strictly?**

(A) **Strict GeoJSON validation for artworks**: Validate that artwork requests conform to GeoJSON Feature spec: `type` must be "Feature", `geometry` must exist with `type: "Point"` and valid `coordinates` array. Reject malformed GeoJSON with clear error messages. For artists, validate the simpler JSON structure.

**Question 32: How should the endpoint handle the source ID from the request?**

(A) **Store in tags, generate new UUID for database**: Take the `id` field from the request (e.g., `"node/publicart117"`), store it in the artwork/artist tags as `source_id`, then generate a new UUID for the database primary key. Return both IDs in the response for tracking.

**Question 33: What should happen if an artist name in the artwork doesn't exist in the database?**

(A) **Auto-create artist stub record**: Create a new artist record with just the name and link it to the artwork via `artwork_artists` table. Set a flag/tag indicating this is an auto-created artist that needs enrichment. Return the created artist ID in the response.

**Question 34: Should the endpoint perform duplicate detection?**

(B) No duplicate detection - always insert new records.

**Question 35: How should the endpoint handle photo uploads?**

**Answer: Updated** - The endpoint receives photo binary data (multipart/form-data) from the CLI, NOT URLs.

(A) **Accept multipart/form-data uploads**: The API endpoint accepts photos as file uploads in the request. Validate file format (JPEG, PNG, WebP), file size (max 15MB), and image dimensions. Process each uploaded photo synchronously: generate hash-based filename, upload to R2 storage, queue thumbnail generation. Return array of photo URLs in response. The endpoint does NOT download from third-party URLs.

**Question 36: What validation should be performed on coordinates?**

(A) **Strict range validation with precision**: Validate lat is between -90 and 90 (inclusive), lon is between -180 and 180 (inclusive). Reject coordinates at exactly (0, 0) or (0.0, 0.0) as likely data errors. Allow up to 8 decimal places for precision. Return clear validation errors.

**Question 37: How should the endpoint handle malformed or missing required fields?**

(A) **Return structured validation errors**: Use Zod or similar validation library. Return 400 Bad Request with detailed field-level errors in a structured format. Include which fields are missing, invalid formats, and expected values. Example: `{ "field": "properties.title", "code": "REQUIRED_FIELD", "message": "Title is required" }`.

**Question 38: Should the endpoint support location enhancement (reverse geocoding)?**

(A) **No - keep endpoint simple**: The endpoint should NOT perform location enhancement. The CLI runner should handle location cache lookups and add location data to the `properties` before sending. This keeps the endpoint fast and stateless.

**Question 39: How should the endpoint handle tags/properties merging?**

(A) **Direct storage with system field preservation**: Store all `properties` from the GeoJSON request directly into the artwork `tags` JSON field. Preserve special system fields like `source`, `source_url`, `source_id`. Don't merge with existing data - this is a new record creation endpoint.

**Question 40: What authentication should the endpoint require?**

(A) **Admin Bearer token validation**: Require `Authorization: Bearer <token>` header. Validate the token against the `users` table and check that the user has `admin` role in the `user_roles` table. Use the system admin UUID (`SYSTEM_ADMIN_USER_UUID`) for audit trail. Return 401 Unauthorized for missing/invalid tokens.

**Question 41: Should the endpoint handle multiple artists per artwork?**

(A) **Yes, parse and create multiple links**: If `properties.artist` is a string, split by comma/semicolon. If `properties.artists` is an array, use it directly. Create or find each artist and create separate `artwork_artists` records. All artists get role `"artist"` by default. Return array of linked artists in response.

**Question 42: How should the endpoint handle database transaction failures?**

One at a time, and report issue as error. no roll back

**Question 43: What rate limiting should the endpoint enforce?**

(C) No rate limiting - it's admin only.

**Question 44: How should the endpoint respond when duplicate is detected?**

(C) Return 200 and create duplicate anyway.

**Question 45: How should the endpoint validate uploaded photos?**

**Answer: Updated** - The endpoint validates binary photo uploads, not URLs.

(A) **Validate uploaded file properties**: Check file format via magic bytes (not just extension), validate MIME type is image/jpeg, image/png, or image/webp, verify file size is under 15MB, check image dimensions are within acceptable range (min: 200x200, max: 4096x4096). Return clear validation errors for invalid uploads. Process valid photos immediately (upload to R2, generate thumbnails).

**Question 46: How should the endpoint handle Markdown in descriptions?**

(A) **Basic sanitization only**: Strip any `<script>` tags and `javascript:` protocols from the Markdown text to prevent XSS. Store the sanitized Markdown as-is in the database. Don't render or convert it - that's the frontend's job. Keep the endpoint simple.

**Question 47: What logging should the endpoint perform?**

(D) Verbose logging of all data, doo not include sensitive info.

**Question 48: How should the endpoint handle artwork types (sculpture, mural, etc.)?**

(A) **Accept and store as-is with optional normalization**: Accept any artwork type string from `properties.artwork_type`. Optionally normalize common variations (e.g., "Sculpture" → "sculpture", "statue" → "sculpture") using a simple mapping table. Store in tags. No strict enum validation.

**Question 49: Should the endpoint support idempotency for retries?**

(B) No idempotency support.

---

## Mass-Import Endpoint Specifications

### Endpoint Requirements Summary

Based on the clarifying questions, here are the recommended specifications:

**Endpoint:** `POST /api/mass-import/v3`

**Key Characteristics:**

- **Stateless**: No session tracking
- **Single-item processing**: One artwork or artist per request
- **Synchronous response**: Returns immediately with result
- **Authenticated**: Requires admin bearer token
- **Atomic operations**: Full rollback on failure
- **Idempotent**: Supports retry safety via idempotency keys

**Input Processing:**

1. Validate authentication (admin token)
2. Determine item type (Feature = artwork, Artist = artist)
3. Validate GeoJSON/JSON structure
4. Validate required fields (coordinates, title, source)
5. Check for duplicates (GPS + source ID)
6. Auto-create artists if needed
7. Store artwork/artist in database with `approved` status
8. Queue photos for async processing
9. Return detailed response

**Response Time Target:** < 500ms (excluding photo downloads)

**Success Criteria:**

- Database record created
- Artists linked
- Photos queued
- Audit log written

### Endpoint Data Flow

```text
Client Request (GeoJSON)
    ↓
Authentication Check
    ↓
Type Detection (artwork vs artist)
    ↓
Schema Validation
    ↓
Duplicate Detection
    ↓
[If duplicate] → Return 200 with skip status
    ↓
[If new] → Begin Transaction
    ↓
Generate UUID for database ID
    ↓
Parse and auto-create artists
    ↓
Create artwork/artist record (status: approved)
    ↓
Create artwork_artists links
    ↓
Queue photos for download
    ↓
Commit Transaction
    ↓
Write audit log
    ↓
Return 200 with created item details
```

### Error Handling Strategy

| Error Type | HTTP Code | Response | Action |
|------------|-----------|----------|---------|
| Missing auth token | 401 | Unauthorized | Return error, no retry |
| Invalid token | 401 | Unauthorized | Return error, no retry |
| Malformed GeoJSON | 400 | Bad Request | Return validation errors |
| Missing required field | 400 | Bad Request | Return field errors |
| Invalid coordinates | 400 | Bad Request | Return validation error |
| Duplicate detected | 200 | Success (skipped) | Return duplicate info |
| Database error | 500 | Internal Error | Rollback, log, return safe error |
| Photo validation failed | 200 | Success (partial) | Create artwork, note photo issues |
| Rate limit exceeded | 429 | Too Many Requests | Return Retry-After header |

### Performance Considerations

**Target Metrics:**

- **Response time**: < 500ms for 95th percentile
- **Throughput**: 100 requests/minute per admin token
- **Database operations**: Single transaction per request
- **Photo validation**: < 3s timeout per photo
- **Duplicate check query**: < 50ms with spatial index

**Optimization Strategies:**

- Use prepared statements for DB queries
- Cache artist lookups (in-memory for request duration)
- Async photo processing (don't block response)
- Efficient spatial indexing for duplicate detection
- Minimal validation (just essentials)

---

## Clarifying Questions - Set 5: Mass-Import CLI Runner

**Question 50: How should the CLI parse and load the input GeoJSON file?**

(B) Load entire file with `JSON.parse()` regardless of size.

**Question 51: How should the CLI handle location cache integration?**

(A) **Pre-process locations before API calls**: Before sending each item to the API, check if coordinates have cached location data. If not in cache, query Nominatim (respecting 1 req/sec limit), cache the result, then add location fields to the item's properties. Send enriched item to API. This keeps the API endpoint simple and stateless.

**Question 52: What command-line arguments should the CLI support?**

(A) **Rich CLI interface with flexibility**: Support arguments: `--input <file>` (required), `--config <file>` (required), `--resume` (optional, resume from checkpoint), `--offset <n>` (skip first n items), `--limit <n>` (process only n items), `--verbose` (detailed logging), `--report-dir <path>` (output directory). Use a CLI framework like `commander` or `yargs`.

**Question 53: How should the CLI display real-time progress to the user?**

(A) **Rich progress indicators**: Use a progress bar library (like `cli-progress` or `ora`) to show: current item number/total, percentage complete, success/failed/skipped counts, current item title, estimated time remaining, items per second. Update every item. For `--verbose` mode, also log each item processed.

**Question 54: Where should the CLI store checkpoint files?**

(A) **Configurable with smart defaults**: Default to `./.mass-import-checkpoints/<sessionId>.json` in current directory. Allow override via config file `checkpointPath`. Create directory if it doesn't exist. Use session ID in filename for multiple concurrent runs. Auto-cleanup old checkpoints after successful completion.

**Question 55: How should the CLI handle resume functionality?**

(A) **Automatic resume detection with user confirmation**: When starting, check if a checkpoint file exists for the same input file. If found, prompt user: "Previous import session found with X items processed. Resume? (y/n)". If yes, load checkpoint and skip already-processed items. If no or `--fresh` flag provided, start from beginning and delete old checkpoint.

**Question 56: How should the CLI validate the configuration file?**

(B) Validate only when each field is used.

**Question 57: What should happen if the input file doesn't exist or is invalid?**

(B) Start processing and fail on first item.

**Question 58: How should the CLI handle network timeouts?**

(A) **Configurable timeouts with sensible defaults**: Use timeout from config file (default: 60000ms). Wrap HTTP requests in timeout wrapper. If request exceeds timeout, treat as retryable error. Increment retry count, wait (exponential backoff), then retry. After max retries (config: maxRetries, default: 3), mark item as failed and continue. Log timeout errors clearly.

**Question 59: Should the CLI support filtering items during import?**

(B) No filtering - import everything.

**Question 60: How should the CLI generate unique session IDs?**

(D) Use input filename as ID with date and timestamp

**Question 61: What information should the CLI log to console vs log file?**

(B) Log everything to both console and file.

**Question 62: How should the CLI handle Markdown sanitization?**

(A) **Sanitize before sending to API**: Use a Markdown sanitizer library (like `sanitize-markdown`) to remove dangerous content (`<script>`, `javascript:`, `onerror` attributes) from description fields. Apply to both artwork descriptions and artist biographies. This prevents XSS attacks. Keep the endpoint simple by doing sanitization client-side.

**Question 63: Should the CLI support parallel/concurrent requests?**

(A) **No concurrency - sequential processing**: Process one item at a time to maintain order, simplify error handling, respect API rate limits, and make progress tracking clear. Use `delayBetweenItems` from config to throttle requests. This is simpler, safer, and sufficient for trusted admin imports.

**Question 64: How should the CLI generate the final report?**

(A) **Multiple format outputs**: Generate both JSON report (machine-readable) and text summary (human-readable). JSON includes all details (see CLI Final Report Format). Text summary shows: input file, duration, counts (total/success/fail/skip), list of failed items with errors. Save both to `--report-dir` (default: `./reports/`) with timestamp. Also log text summary to console.

**Question 65: What should the CLI do if the API returns unexpected status codes?**

(A) **Categorized error handling**:

- 200/201: Success (parse response)
- 400: Validation error (log, mark failed, continue)
- 401: Auth error (fail entire import - bad token)
- 429: Rate limit (wait and retry with backoff)
- 500: Server error (retry up to maxRetries, then fail item)
- Other: Log unexpected code, retry, then fail item

Use clear error messages for each category.

**Question 66: How should the CLI handle artist auto-creation tracking?**

(A) **Track in session statistics**: Maintain separate counters for artists: `created` (new artist records), `linked` (existing artists linked to artworks), `auto-created` (stub artists created from artwork imports). Include in final report. This helps users understand what artist data was added vs what already existed.

**Question 67: Should the CLI validate data before sending to API?**

(A) **Basic pre-flight validation**: Check required fields exist (coordinates, title, source), validate coordinate ranges (-90 to 90, -180 to 180), validate URL formats for photos and source_url. If validation fails, skip item and mark as failed with clear error message. This catches obvious errors early and reduces API load.

**Question 68: How should the CLI download and process photos?**

**Answer: Updated** - The CLI is responsible for downloading photos from source URLs and preparing them for upload.

(A) **Download, validate, cache, and upload**: For each photo URL in the scraper output:

1. Download photo from source URL (with retry logic, 3 attempts)
2. Validate downloaded file (check magic bytes, format, size)
3. Cache locally using content hash as filename (e.g., `.cache/photos/<hash>.jpg`)
4. On subsequent runs, check cache first before re-downloading
5. Include photo binary data in multipart/form-data request to API
6. Track download statistics (successful, failed, cached)
7. If download fails after retries, log error and continue (don't fail entire item)

This keeps the API simple and gives the CLI control over download retries and caching.

**Question 69: What exit codes should the CLI use?**

(B) Only 0 (success) or 1 (any error).

---

## CLI Runner Specifications

### CLI Command Structure

**Basic Usage:**

```bash
mass-import-cli --input ./data/artworks.geojson --config ./config/production.json
```

**Full Options:**

```bash
mass-import-cli \
  --input <path>           # Input file (GeoJSON or JSON)
  --config <path>          # Configuration file (JSON)
  --resume                 # Resume from checkpoint
  --offset <number>        # Skip first N items
  --limit <number>         # Process only N items
  --verbose                # Detailed logging
  --report-dir <path>      # Report output directory
  --fresh                  # Ignore existing checkpoint, start fresh
```

### CLI Workflow

```text
Start CLI
    ↓
Parse arguments
    ↓
Load & validate configuration
    ↓
Check input file exists & is valid format
    ↓
Generate session ID
    ↓
Check for existing checkpoint (if --resume)
    ↓
Initialize location cache (if enabled)
    ↓
Parse input file (streaming)
    ↓
For each item:
    ├─ Check if in checkpoint (skip if already processed)
    ├─ Validate required fields
    ├─ Enhance with location data (if enabled)
    ├─ Sanitize Markdown
    ├─ Send to API endpoint
    ├─ Handle response (success/error/retry)
    ├─ Update statistics
    ├─ Write checkpoint
    └─ Update progress display
    ↓
Generate final reports (JSON + text)
    ↓
Display summary
    ↓
Exit with appropriate code
```

### CLI Session Lifecycle

**1. Initialization Phase:**

- Validate arguments and config
- Create session ID
- Initialize log file
- Load checkpoint (if resuming)
- Display configuration summary

**2. Processing Phase:**

- Stream input file
- Process items sequentially
- Track statistics in real-time
- Update progress bar
- Write checkpoints periodically

**3. Completion Phase:**

- Generate final report files
- Display summary to console
- Cleanup resources
- Delete checkpoint (if successful)
- Exit with code

### Error Recovery Strategy

| Error Type | CLI Action | Retry | Continue |
|------------|------------|-------|----------|
| Config validation | Show error, exit 1 | No | No |
| Input file not found | Show error, exit 1 | No | No |
| Auth failure (401) | Show error, exit 1 | No | No |
| Network timeout | Exponential backoff | Yes (3x) | Yes if retries exhausted |
| API validation (400) | Log error, mark failed | No | Yes |
| Server error (500) | Exponential backoff | Yes (3x) | Yes if retries exhausted |
| Rate limit (429) | Wait (Retry-After) | Yes (10x) | Yes |
| Location cache error | Log warning, skip location | No | Yes |

### CLI Output Examples

**Progress Display (Normal Mode):**

```text
Mass Import v3 - Session: import-20251014-143052-a7b3
Input: ./data/burnaby-artworks.geojson
Config: ./config/production.json

Processing: ████████████████░░░░ 80% | 120/150
Success: 115 | Failed: 5 | Skipped: 0
Current: "Blacktail" by Muse Atelier
Speed: 3.2 items/sec | ETA: 2m 15s
```

**Final Summary:**

```text
✅ Import Complete!

Session: import-20251014-143052-a7b3
Duration: 45m 23s
Input: ./data/burnaby-artworks.geojson

Results:
  Total Items: 150
  ✓ Successful: 143
  ✗ Failed: 7
  ⊘ Skipped: 0

Details:
  Artworks Created: 115
  Artists Created: 12
  Artists Linked: 18
  Photos Processed: 240

Performance:
  Items/Second: 3.3
  API Calls: 150
  Retries: 12

Reports saved to:
  - ./reports/import-20251014-143052-a7b3.json
  - ./reports/import-20251014-143052-a7b3.txt

❌ 7 items failed - see report for details
```

### CLI Dependencies

**Required Node.js Packages:**

- `commander` or `yargs` - CLI argument parsing
- `cli-progress` or `ora` - Progress bars and spinners
- `JSONStream` or similar - Streaming JSON parser
- `better-sqlite3` - Location cache
- `node-fetch` or `axios` - HTTP requests
- `zod` - Configuration validation
- `sanitize-markdown` - Markdown sanitization
- `chalk` - Terminal colors
- `winston` or `pino` - Structured logging

**Optional:**

- `dotenv` - Environment variable support
- `prompts` - Interactive confirmations
- `table` - Pretty console tables

---

## Clarifying Questions - Set 6: Scrapers

**Question 70: What programming language/framework should scrapers be built with?**

(B) All scrapers must use the same language/framework, NodeJS

**Question 71: How should scrapers be organized in the project structure?**

(D) No specific organization - ad-hoc scripts.

**Question 72: Should scrapers include retry logic for failed requests?**

(A) **Yes, with exponential backoff**: Implement retry logic for HTTP requests (3-5 attempts with exponential backoff starting at 1s). Handle common errors: network timeouts, 429 rate limits, 500 server errors. Log retry attempts. This makes scrapers resilient to temporary failures and polite to source servers.

**Question 73: How should scrapers handle rate limiting to avoid overwhelming source websites?**

(A) **Respectful rate limiting with delays**: Implement delay between requests (configurable, default 1-2 seconds). Respect `robots.txt` and `Retry-After` headers. Use random jitter to avoid patterns. Consider time-of-day (avoid peak hours if scraping large datasets). Log request rates. This maintains good relationship with data sources and avoids being blocked.

**Question 74: Should scrapers validate data before writing to output files?**

(A) **Basic validation with warnings**: Validate essential fields (coordinates are numbers in valid range, title exists, source_url is valid URL). Warn about missing optional fields (description, photos). Include validation report in scraper output. Write valid items to output file, skip invalid items with error log. This catches scraper bugs early.

**Question 75: How should scrapers handle incremental updates (only scraping new/changed items)?**

(C) Manual tracking by user.

**Question 76: What metadata should scrapers include in the output files?**

(A) **Include scraper metadata in output**: Add metadata section to output files with: scraper name/version, source website URL, scrape timestamp, total items scraped, scraper configuration used. For GeoJSON, use top-level properties. This helps track data provenance and debug issues.

**Question 77: How should scrapers handle pagination on source websites?**

(A) **Auto-detect and follow pagination**: Detect pagination patterns (page numbers, "next" links, infinite scroll). Follow pagination links until no more pages. Support configurable page limits for testing (e.g., `--max-pages 5`). Log page progress. Handle different pagination styles (offset, cursor, page number). This ensures complete data capture.

**Question 78: Should scrapers normalize data or preserve source formatting?**

(C) Heavy normalization to match database schema exactly.

**Question 79: How should scrapers handle missing or incomplete data?**

(A) **Partial data with clear marking**: Include items with partial data, marking missing fields as null. Add `data_completeness` tag with percentage of expected fields populated. Log items with missing critical fields (coordinates, title). Let CLI/API validation decide if item is acceptable. This captures maximum data while maintaining quality visibility.

**Question 80: Should scrapers extract image URLs or download images directly?**

(A) **Extract URLs only, CLI downloads**: Scrapers should extract and include photo URLs in the output GeoJSON/JSON. Don't download images - let the CLI handle downloading, validation, and caching. This keeps scrapers fast and simple. Include all available photo URLs (high-res preferred). Validate URLs are properly formatted.

**Question 81: How should scrapers handle different coordinate formats (DMS, UTM, etc.)?**

(A) **Convert to decimal degrees**: Detect coordinate format (DMS, UTM, MGRS, etc.) and convert to decimal degrees (lat/lon). 

**Question 82: What error logging should scrapers implement?**

(A) **Structured error logging with levels**: Log to both console and file (`scraper-<timestamp>.log`). Include levels: DEBUG (request details), INFO (progress), WARN (missing optional data), ERROR (failed requests, invalid data). Use structured logging (JSON format option). Include context (URL being scraped, item ID). This aids debugging and monitoring.

**Question 83: Should scrapers support configuration files?**

CLI with readme.md with full example of cli useage

**Question 84: How should scrapers handle authentication (if required by source)?**

(B) No authentication support - public data only.

**Question 85: Should scrapers generate preview/validation reports?**

(C) Only error reports.

**Question 86: How should scrapers handle duplicate detection within scraped data?**

(A) **De-duplicate based on unique identifiers**: Use source's unique ID field (if available) or generate from combination of fields (title + location). Track seen items, skip duplicates within same scrape. Log duplicate count. Let mass-import system handle duplicates across different sources. This prevents importing same item multiple times from pagination/multi-page sources.

**Question 87: Should scrapers support dry-run mode?**

(B) No dry-run - always write files.

**Question 88: How should scrapers document their data source and requirements?**

(A) **Comprehensive README per scraper**: Each scraper directory includes README.md with:
- Source website description and URL
- Data fields available
- Known limitations/quirks
- Setup instructions (dependencies, credentials)
- Usage examples
- Last successful scrape date
- Maintenance notes (changes to source website)
This helps future maintainers and users understand each scraper.

**Question 89: Should scrapers handle multi-language content?**

(B) Only scrape English content.

---

## Scraper Specifications

### Scraper Output Requirements

**GeoJSON Artwork File Format:**

```json
{
  "type": "FeatureCollection",
  "metadata": {
    "scraper": "burnaby-art-gallery-scraper",
    "version": "1.0.0",
    "source": "https://burnabyartgallery.ca",
    "scrapedAt": "2025-10-14T12:00:00Z",
    "totalItems": 150
  },
  "features": [
    {
      "type": "Feature",
      "id": "source-unique-id-here",
      "geometry": {
        "type": "Point",
        "coordinates": [-123.003613, 49.225237]
      },
      "properties": {
        "source": "https://burnabyartgallery.ca",
        "source_url": "https://collections.burnabyartgallery.ca/link/publicart117",
        "title": "blacktail",
        "description": "Artwork description in Markdown...",
        "artwork_type": "sculpture",
        "artist": "Muse Atelier",
        "start_date": "2015",
        "material": "aluminum",
        "photos": [
          "https://collections.burnabyartgallery.ca/path/to/photo1.jpg",
          "https://collections.burnabyartgallery.ca/path/to/photo2.jpg"
        ]
      }
    }
  ]
}
```

**JSON Artist File Format:**

```json
{
  "metadata": {
    "scraper": "burnaby-art-gallery-scraper",
    "version": "1.0.0",
    "source": "https://burnabyartgallery.ca",
    "scrapedAt": "2025-10-14T12:00:00Z",
    "totalItems": 30
  },
  "artists": [
    {
      "type": "Artist",
      "id": "source-unique-id-here",
      "name": "Muse Atelier",
      "description": "Artist biography in Markdown...",
      "properties": {
        "source": "https://burnabyartgallery.ca",
        "source_url": "https://collections.burnabyartgallery.ca/link/artists1307",
        "birth_date": "1928",
        "death_date": "2016",
        "website": "http://www.example.com"
      }
    }
  ]
}
```

### Scraper Development Guidelines

**Essential Features:**

- HTTP request handling with retry logic
- Rate limiting (1-2 second delays)
- Pagination support
- Data validation
- Error logging
- Progress indicators
- Configuration file support

**Optional Features:**

- Incremental scraping
- Dry-run mode
- Authentication support
- Multi-language handling
- Preview reports

**Code Quality:**

- TypeScript or Python with type hints
- Clear error messages
- Logging at appropriate levels
- Comments for complex logic
- README with usage examples

**Testing:**

- Test with small dataset first (`--max-pages 2`)
- Validate output file format
- Check coordinate accuracy
- Verify photo URLs are accessible
- Review scrape report before mass import


